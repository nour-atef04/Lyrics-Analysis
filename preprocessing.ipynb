{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music and lyrics subtly shape our emotions, thoughts, and perceptions. The growing field of lyric analysis in data science is beginning to gain attention in academic research, exploring patterns in songwriting and even predicting musical success. This case study will touch upon some of these emerging ideas while focusing on the extensive discography of one of the most legendary and influential Egyptian artists, Amr Diab. \n",
    "\n",
    "With a career spanning over four decades, Amr Diab, also known as \"El Hadaba\", has shaped the modern Arabic music scene with his fusion of traditional Arabic melodies and contemporary Western influences that resonates across generations. He is regarded as one of the pioneers of modern Arabic pop music, resulting in him dominating the Arabic music charts while winning multiple World Music Awards. His songs cover a diverse range of themes, from love and nostalgia to personal growth and cultural identity. \n",
    "\n",
    "In this project, we will explore the potential insights that can be drawn from Amr Diab’s songwriting. From common words and themes to sentiment trends over time, we aim to shed light into the artist's remarkable contribution to Arabic music through a data-driven lens, using statistical analysis, natural language processing (NLP), and visualization techniques.\n",
    "\n",
    "The dataset consists of lyrics collected from Genius.com, ensuring a comprehensive and accurate representation of Amr Diab’s discography. This study will provide insights into his artistic evolution and lyrical tendencies, contributing to a deeper appreciation of his music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Colletion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of all songs, with their corresponding lyrics, were collected using https://genius.com/artists/Amr-diab, and made into a .csv file, after carefully examining the lyrics for any inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section imports the necessary libraries for handling and preprocessing Arabic text. It begins with Pandas, a powerful library for managing structured data, and re, which provides regular expression support for text processing.\n",
    "\n",
    "In this project, we will be mostly using **Camel Tools** since it is a specialized NLP library designed for Arabic language processing. Therefore, unlike general-purpose NLP libraries, Camel Tools provides linguistically informed preprocessing functions tailored for Arabic.\n",
    "\n",
    "Tools we'll be using here:\n",
    "- Unicode normaliztion\n",
    "- Text normaliztion: for example, converting ا → أ, إ, آ\n",
    "- Diacritic Removal (harakat)\n",
    "- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#%pip install camel-tools\n",
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Composer</th>\n",
       "      <th>Lyricist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد أحمد فؤاد</td>\n",
       "      <td>تامر حسين</td>\n",
       "      <td>بيوحشنا</td>\n",
       "      <td>ملازمنا، ملازمنا خياله وطيفه فين ما نروح\\nملاز...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>أحمد إبراهيم</td>\n",
       "      <td>أيمن بهجت قمر</td>\n",
       "      <td>معرفش حد بالأسم ده</td>\n",
       "      <td>ما أعرفش حد بالإسم دا\\nأنا اللي تاه عقله ولقاه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد يحيي</td>\n",
       "      <td>بهاء الدين محمد</td>\n",
       "      <td>ظبط مودها</td>\n",
       "      <td>لما تظبط مودها\\nأطلب حتى عينيها تاخدها\\nوأؤمر ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد يحيي</td>\n",
       "      <td>محمد القاياتي</td>\n",
       "      <td>سلامك وصلي</td>\n",
       "      <td>سلامك وصلي\\nوأتاريني واحشك زي ما إنت واحشني يا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد يحيي</td>\n",
       "      <td>محمد البوغة</td>\n",
       "      <td>واخدين راحتهم</td>\n",
       "      <td>واخذين راحتهم قاعدين في قلبي مربعين وبيعصروه\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>1983</td>\n",
       "      <td>هاني شنودة</td>\n",
       "      <td>هاني ذكي</td>\n",
       "      <td>الزمن</td>\n",
       "      <td>الزمن بينسى دايماً، مع الزمن مفيش وعود\\nاللي ك...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>1983</td>\n",
       "      <td>هاني شنودة</td>\n",
       "      <td>عبد الرحيم منصور</td>\n",
       "      <td>نور يا ليل</td>\n",
       "      <td>نور يا ليل الأسرار\\nيا اللي عشقناك وإحنا صغار\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>1983</td>\n",
       "      <td>عزمي الكيلاني</td>\n",
       "      <td>عصام عبدالله</td>\n",
       "      <td>وقت وعشناه</td>\n",
       "      <td>وقت وعشناه إنتي وأنا، جرح حفرناه لبقية عمرنا\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>1983</td>\n",
       "      <td>ياسر عبد الحليم</td>\n",
       "      <td>عوض الرخاوي</td>\n",
       "      <td>أحلى دنيا</td>\n",
       "      <td>إمتى نشوف البسمة الحلوة\\nمالية شفايف كل الناس\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1983</td>\n",
       "      <td>هاني شنودة</td>\n",
       "      <td>هاني ذكي</td>\n",
       "      <td>أحضان الجبل</td>\n",
       "      <td>في يوم والشمس طالعة بألوانها الرقيقة ولمستها ا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year         Composer          Lyricist                Song  \\\n",
       "0    2023   محمد أحمد فؤاد         تامر حسين             بيوحشنا   \n",
       "1    2023     أحمد إبراهيم     أيمن بهجت قمر  معرفش حد بالأسم ده   \n",
       "2    2023        محمد يحيي   بهاء الدين محمد           ظبط مودها   \n",
       "3    2023        محمد يحيي     محمد القاياتي          سلامك وصلي   \n",
       "4    2023        محمد يحيي       محمد البوغة       واخدين راحتهم   \n",
       "..    ...              ...               ...                 ...   \n",
       "305  1983       هاني شنودة          هاني ذكي               الزمن   \n",
       "306  1983       هاني شنودة  عبد الرحيم منصور          نور يا ليل   \n",
       "307  1983    عزمي الكيلاني      عصام عبدالله          وقت وعشناه   \n",
       "308  1983  ياسر عبد الحليم       عوض الرخاوي           أحلى دنيا   \n",
       "309  1983       هاني شنودة          هاني ذكي         أحضان الجبل   \n",
       "\n",
       "                                                Lyrics  \n",
       "0    ملازمنا، ملازمنا خياله وطيفه فين ما نروح\\nملاز...  \n",
       "1    ما أعرفش حد بالإسم دا\\nأنا اللي تاه عقله ولقاه...  \n",
       "2    لما تظبط مودها\\nأطلب حتى عينيها تاخدها\\nوأؤمر ...  \n",
       "3    سلامك وصلي\\nوأتاريني واحشك زي ما إنت واحشني يا...  \n",
       "4    واخذين راحتهم قاعدين في قلبي مربعين وبيعصروه\\n...  \n",
       "..                                                 ...  \n",
       "305  الزمن بينسى دايماً، مع الزمن مفيش وعود\\nاللي ك...  \n",
       "306  نور يا ليل الأسرار\\nيا اللي عشقناك وإحنا صغار\\...  \n",
       "307  وقت وعشناه إنتي وأنا، جرح حفرناه لبقية عمرنا\\n...  \n",
       "308  إمتى نشوف البسمة الحلوة\\nمالية شفايف كل الناس\\...  \n",
       "309  في يوم والشمس طالعة بألوانها الرقيقة ولمستها ا...  \n",
       "\n",
       "[310 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define file paths\n",
    "DATA_PATH = \"./data/amr_diab_songs.csv\"\n",
    "STOP_WORDS_PATH = \"./data/stop_words.txt\"\n",
    "PREPROC_PATH = \"./data/amr_diab_songs_proc.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "songs = pd.read_csv(DATA_PATH, header=0, encoding=\"utf-8\", na_values=\"\")\n",
    "\n",
    "songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also load a predefined list of **stopwords**, because we'll be removing them from the lyrics of each song later on in this notebook. \n",
    "\n",
    "When it comes to data mining in NLP, people often have a common belief. \"The most frequent words are the most important\". This seems logical at first, but in reality, it's usually the less frequent words that carry more meaning. Think about it: in a news article, words like “breaking” or “crisis” are way more insightful than generic words like “the” or “is.” \n",
    "\n",
    "These common generic words are called **stopwords**, and they typically do not carry significant meaning and are often removed during text preprocessing NLP tasks. In Arabic, stopwords include words like \"و\" (and), \"في\" (in), and \"على\" (on), which appear frequently but contribute little to understanding the main content of a text. Hence, removing stopwords will help reduce noise, improve computational efficiency, and enhance the performance of machine learning models by focusing on more meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>،</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ء</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اذار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11009</th>\n",
       "      <td>بكام</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11010</th>\n",
       "      <td>لكام</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11011</th>\n",
       "      <td>يل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11012</th>\n",
       "      <td>ويلا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11013</th>\n",
       "      <td>فيل</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11014 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word\n",
       "0         ،\n",
       "1         ء\n",
       "2         ا\n",
       "3        اب\n",
       "4      اذار\n",
       "...     ...\n",
       "11009  بكام\n",
       "11010  لكام\n",
       "11011    يل\n",
       "11012  ويلا\n",
       "11013   فيل\n",
       "\n",
       "[11014 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load stopwords\n",
    "my_stopwords = pd.read_csv(STOP_WORDS_PATH, header=None, names=[\"Word\"], encoding=\"utf-8\")\n",
    "stopwords_set = set(my_stopwords[\"Word\"]) # Convert stopwords into a set for faster lookup\n",
    "\n",
    "my_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure data quality, we remove any rows containing missing values, preventing potential errors during further analysis. Then, we confirm that no missing values remain by displaying the count of null values in each column. These steps help clean and prepare the dataset for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 310 entries, 0 to 309\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Year      310 non-null    int64 \n",
      " 1   Composer  309 non-null    object\n",
      " 2   Lyricist  309 non-null    object\n",
      " 3   Song      310 non-null    object\n",
      " 4   Lyrics    310 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 12.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Year        0\n",
       "Composer    0\n",
       "Lyricist    0\n",
       "Song        0\n",
       "Lyrics      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "songs.info()\n",
    "\n",
    "# Drop rows with any missing data\n",
    "songs.dropna(inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "songs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = re.findall(r'\\w+', text)  \n",
    "    return \" \".join(word for word in words if word not in stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Removing Stop Words: أنا اللي تاه عقله، و لقاه ما أعرفش حد بالإسم دا موهوم       بقى وربك هداه هعتبره وقت فراغ و فاتABC 123\n",
      "After Removing Stop Words: أنا تاه عقله لقاه أعرفش بالإسم موهوم بقى وربك هداه هعتبره وقت فراغ فاتABC 123\n"
     ]
    }
   ],
   "source": [
    "sample_text = 'أنا اللي تاه عقله، و لقاه ما أعرفش حد بالإسم دا موهوم       بقى وربك هداه هعتبره وقت فراغ و فاتABC 123'\n",
    "\n",
    "print(\"Before Removing Stop Words:\", sample_text)\n",
    "sample_text = remove_stopwords(sample_text)\n",
    "print(\"After Removing Stop Words:\", remove_stopwords(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output here demonstrates the effect of the **remove_stopwords** function on a sample Arabic text. \n",
    "\n",
    "The original text includes common stopwords such as \"اللي\" (which), \"ما\" (not), \"حد\" (someone), and \"دا\" (this), as well as English letters (ABC) and numbers (123) at the end. The filtered text retains meaningful words while removing stopwords like \"اللي\", \"ما\", \"حد\", and \"دا\". However, numbers (\"123\") and English text (\"ABC\") remain. Hence, further processing will be needed to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **clean_text** function here performs a series of preprocessing steps to clean and normalize Arabic text for NLP tasks:\n",
    "- It first ensures the input is a string and removes English characters, punctuation, numbers, and extra spaces using regular expressions. \n",
    "- Then, it applies Unicode normalization to standardize character encoding, followed by orthographic normalization to unify Arabic letter variations, such as different forms of \"Alef\" (ا, أ, إ) and \"Teh Marbuta\" (ة to ه). \n",
    "- Then it addresses text elongation by reducing repeated letters and manually correcting common cases like \"وو\" to \"و\" and \"يي\" to \"ي\". \n",
    "- Finally, it removes diacritics to simplify the text and make it more uniform for analysis. These steps help improve text consistency, reduce noise, and enhance the performance of NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)  # Ensure input is a string\n",
    "    text = re.sub(r\"[A-Za-z]\", \"\", text)  # Remove English text\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "\n",
    "    # Unicode normalization\n",
    "    text = normalize_unicode(text)  \n",
    "\n",
    "    # Orthographic normalization\n",
    "    text = normalize_alef_ar(text)\n",
    "    text = normalize_alef_maksura_ar(text)\n",
    "    text = normalize_teh_marbuta_ar(text)\n",
    "\n",
    "    # Remove longation (repeated letters)\n",
    "    p_longation = re.compile(r\"(.)\\1+\")\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "\n",
    "    # Fix repeated letters\n",
    "    text = text.replace(\"وو\", \"و\")\n",
    "    text = text.replace(\"يي\", \"ي\")\n",
    "    text = text.replace(\"اا\", \"ا\")\n",
    "\n",
    "    # Remove diacritics\n",
    "    text = dediac_ar(text)  \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning: أنا تاه عقله لقاه أعرفش بالإسم موهوم بقى وربك هداه هعتبره وقت فراغ فاتABC 123\n",
      "After Cleaning: انا تاه عقله لقاه اعرفش بالاسم موهوم بقي وربك هداه هعتبره وقت فراغ فات\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Cleaning:\", sample_text)\n",
    "sample_text = clean_text(sample_text)\n",
    "print(\"After Cleaning:\", sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output here shows the effect of the text cleaning function on the given Arabic text.\n",
    "\n",
    "Before cleaning, the text contains Arabic words, English characters (\"ABC\"), numbers (\"123\"), diacritics, and different forms of Arabic letters (e.g., \"الإسم\" with hamza and alef-lam).\n",
    "\n",
    "After cleaning:\n",
    "- English characters and numbers are removed (\"ABC 123\" is gone).\n",
    "- Diacritics are stripped, making the text simpler and more consistent.\n",
    "- Certain Arabic letters are normalized, such as \"الإسم\" → \"الاسم\" and \"بقي\" → \"بقى\" to standardize spelling variations.\n",
    "\n",
    "This preprocessing step is essential for improving text consistency and preparing it for further NLP tasks, such as tokenization or analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization** is a crucial preprocessing step in NLP that involves breaking text into smaller units called **tokens**, which can be words, phrases, or subwords. This process standardizes text and makes it easier to analyze by separating words from punctuation, special characters, and unnecessary elements. Tokenization enables models to process and understand text effectively, and it plays a key role in improving machine learning performance by converting text into structured representations suitable for embedding and vectorization.We use Camel Tools’ simple_word_tokenize() help handle these challenges by efficiently segmenting Arabic text while preserving linguistic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return simple_word_tokenize(text)    # Returns a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization: انا تاه عقله لقاه اعرفش بالاسم موهوم بقي وربك هداه هعتبره وقت فراغ فات\n",
      "After Tokenization: ['انا', 'تاه', 'عقله', 'لقاه', 'اعرفش', 'بالاسم', 'موهوم', 'بقي', 'وربك', 'هداه', 'هعتبره', 'وقت', 'فراغ', 'فات']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Tokenization:\", sample_text)\n",
    "sample_text = tokenize_text(sample_text)\n",
    "print(\"After Tokenization:\", sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text processing in Arabic is highly important because the language has its own unique challenges that make NLP a bit trickier here than in English. Arabic is highly inflected, meaning words change a lot depending on tense, gender, and case, as well as diacritics and different script forms.\n",
    "\n",
    "For example, take the short vowel marks like \"َ\" (fatha) or \"ِ\" (kasra) that can completely change a word’s meaning. For instance, \"عَلَم\" (ʿalam) means \"flag,\" while \"عِلْم\" (ʿilm) means \"knowledge.\" Since diacritics are often omitted in writing, text processing needs to either restore them or develop methods to handle the ambiguity.\n",
    "\n",
    "Another challenge is the spelling normalization. Arabic has multiple ways to write the same word due to variations in \"Alif\" (ا vs. أ vs. إ) and \"Ya\" (ي vs. ى). If we don’t normalize them, the model might treat \"مسؤولية\" and \"مسئولية\" (both meaning \"responsibility\") as separate words!\n",
    "\n",
    "All of these complexities make text preprocessing crucial in Arabic NLP. By careful text processing, we help models focus on the meaning of words rather than getting lost in small variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_normalized(text):\n",
    "    \"\"\"Finds and returns non-normalized Arabic letters and diacritics in the text.\"\"\"\n",
    "    non_normalized_chars = {\n",
    "        \"أ\": \"Alef with Hamza Above\",\n",
    "        \"إ\": \"Alef with Hamza Below\",\n",
    "        \"آ\": \"Alef with Madda\",\n",
    "        \"ى\": \"Final Yeh (should be ي)\",\n",
    "        \"ة\": \"Teh Marbuta (should be ه)\"\n",
    "    }\n",
    "    \n",
    "    diacritics_pattern = r\"[\\u064B-\\u065F]\"  # Arabic diacritics (Harakat)\n",
    "\n",
    "    found_chars = {}\n",
    "\n",
    "    # Check for non-normalized letters\n",
    "    for char, desc in non_normalized_chars.items():\n",
    "        count = text.count(char)\n",
    "        if count > 0:\n",
    "            found_chars[char] = (desc, count)\n",
    "\n",
    "    # Check for diacritics\n",
    "    diacritics_matches = re.findall(diacritics_pattern, text)\n",
    "    if diacritics_matches:\n",
    "        found_chars[\"Diacritics\"] = (\"Arabic Harakat\", len(diacritics_matches))\n",
    "\n",
    "    return found_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أ (Alef with Hamza Above): 5092 occurrences\n",
      "إ (Alef with Hamza Below): 3389 occurrences\n",
      "ى (Final Yeh (should be ي)): 2269 occurrences\n",
      "ة (Teh Marbuta (should be ه)): 3405 occurrences\n",
      "Diacritics (Arabic Harakat): 204 occurrences\n",
      "آ (Alef with Madda): 794 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Apply to the original dataset\n",
    "non_normalized_counts = songs[\"Lyrics\"].apply(find_non_normalized)\n",
    "\n",
    "# Aggregate occurrences across all lyrics\n",
    "summary = {}\n",
    "for entry in non_normalized_counts:\n",
    "    for key, value in entry.items():\n",
    "        if key in summary:\n",
    "            summary[key] = (value[0], summary[key][1] + value[1])\n",
    "        else:\n",
    "            summary[key] = value\n",
    "\n",
    "# Display results\n",
    "for char, (desc, count) in summary.items():\n",
    "    print(f\"{char} ({desc}): {count} occurrences\")\n",
    "\n",
    "if len(summary) == 0:\n",
    "    print(\"Lyrics are now clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Composer</th>\n",
       "      <th>Lyricist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد أحمد فؤاد</td>\n",
       "      <td>تامر حسين</td>\n",
       "      <td>بيوحشنا</td>\n",
       "      <td>[ملازمنا, ملازمنا, خياله, وطيفه, فين, نروح, مل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>أحمد إبراهيم</td>\n",
       "      <td>أيمن بهجت قمر</td>\n",
       "      <td>معرفش حد بالأسم ده</td>\n",
       "      <td>[اعرفش, بالاسم, تاه, عقله, ولقاه, اعرفش, بالاس...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد يحيي</td>\n",
       "      <td>بهاء الدين محمد</td>\n",
       "      <td>ظبط مودها</td>\n",
       "      <td>[تظبط, مودها, اطلب, عينيها, تاخدها, واؤمر, واح...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد يحيي</td>\n",
       "      <td>محمد القاياتي</td>\n",
       "      <td>سلامك وصلي</td>\n",
       "      <td>[سلامك, وصلي, واتاريني, واحشك, واحشني, وهتفضل,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد يحيي</td>\n",
       "      <td>محمد البوغة</td>\n",
       "      <td>واخدين راحتهم</td>\n",
       "      <td>[واخذين, راحتهم, قاعدين, قلبي, مربعين, وبيعصرو...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year        Composer         Lyricist                Song  \\\n",
       "0  2023  محمد أحمد فؤاد        تامر حسين             بيوحشنا   \n",
       "1  2023    أحمد إبراهيم    أيمن بهجت قمر  معرفش حد بالأسم ده   \n",
       "2  2023       محمد يحيي  بهاء الدين محمد           ظبط مودها   \n",
       "3  2023       محمد يحيي    محمد القاياتي          سلامك وصلي   \n",
       "4  2023       محمد يحيي      محمد البوغة       واخدين راحتهم   \n",
       "\n",
       "                                              Lyrics  \n",
       "0  [ملازمنا, ملازمنا, خياله, وطيفه, فين, نروح, مل...  \n",
       "1  [اعرفش, بالاسم, تاه, عقله, ولقاه, اعرفش, بالاس...  \n",
       "2  [تظبط, مودها, اطلب, عينيها, تاخدها, واؤمر, واح...  \n",
       "3  [سلامك, وصلي, واتاريني, واحشك, واحشني, وهتفضل,...  \n",
       "4  [واخذين, راحتهم, قاعدين, قلبي, مربعين, وبيعصرو...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply text preprocessing\n",
    "songs[\"Lyrics\"] = songs[\"Lyrics\"].apply(remove_stopwords).apply(clean_text).apply(remove_stopwords).apply(tokenize_text)\n",
    "\n",
    "# Display processed data\n",
    "songs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics are now clean\n"
     ]
    }
   ],
   "source": [
    "# Apply to the tokenized dataset\n",
    "non_normalized_counts = songs[\"Lyrics\"].apply(lambda tokens: find_non_normalized(\" \".join(tokens)))\n",
    "\n",
    "# Aggregate occurrences across all lyrics\n",
    "summary = {}\n",
    "for entry in non_normalized_counts:\n",
    "    for key, value in entry.items():\n",
    "        if key in summary:\n",
    "            summary[key] = (value[0], summary[key][1] + value[1])\n",
    "        else:\n",
    "            summary[key] = value\n",
    "\n",
    "# Display results\n",
    "for char, (desc, count) in summary.items():\n",
    "    print(f\"{char} ({desc}): {count} occurrences\")\n",
    "\n",
    "if len(summary) == 0:\n",
    "    print(\"Lyrics are now clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing Decades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of categorizing decades here is to help with analysis of trends and patterns in Amr Diab's songs across different time periods. By grouping his songs based on the decade they were released, we can observe how musical styles, lyrical themes, and composer-lyricist collaborations have evolved over time. \n",
    "\n",
    "This categorization also allows for comparative analysis, such as understanding shifts in lyrical complexity, or examining how external cultural and technological factors influenced music production. \n",
    "\n",
    "Additionally, grouping by decades aids in data visualization, making it easier to spot overarching trends in the dataset, such as the rise of certain genres or the dominance of specific composers in different periods. This level of categorization provides a structured way to analyze and interpret historical musical data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_decade(year):\n",
    "    if 1980 <= year <= 1984:\n",
    "        return \"Early 1980s\"\n",
    "    elif 1985 <= year <= 1989:\n",
    "        return \"Late 1980s\"\n",
    "    elif 1990 <= year <= 1994:\n",
    "        return \"Early 1990s\"\n",
    "    elif 1995 <= year <= 1999:\n",
    "        return \"Late 1990s\"\n",
    "    elif 2000 <= year <= 2004:\n",
    "        return \"Early 2000s\"\n",
    "    elif 2005 <= year <= 2009:\n",
    "        return \"Late 2000s\"\n",
    "    elif 2010 <= year <= 2014:\n",
    "        return \"Early 2010s\"\n",
    "    elif 2015 <= year <= 2019:\n",
    "        return \"Late 2010s\"\n",
    "    elif 2020 <= year <= 2025:\n",
    "        return \"Early 2020s\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize songs by decade\n",
    "songs[\"Decade\"] = songs[\"Year\"].apply(categorize_decade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_name(name):\n",
    "    if pd.isna(name):\n",
    "        return None, None\n",
    "    parts = name.split()\n",
    "    first_name = parts[0]\n",
    "    last_name = \" \".join(parts[1:]) if len(parts) > 1 else None\n",
    "    return first_name, last_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract first and last names\n",
    "songs[\"Composer_first_name\"], songs[\"Composer_last_name\"] = zip(*songs[\"Composer\"].apply(split_name))\n",
    "songs[\"Lyricist_first_name\"], songs[\"Lyricist_last_name\"] = zip(*songs[\"Lyricist\"].apply(split_name))\n",
    "#songs.drop(columns=[\"Year\", \"Composer\", \"Lyricist\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Composer</th>\n",
       "      <th>Lyricist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Decade</th>\n",
       "      <th>Composer_first_name</th>\n",
       "      <th>Composer_last_name</th>\n",
       "      <th>Lyricist_first_name</th>\n",
       "      <th>Lyricist_last_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد أحمد فؤاد</td>\n",
       "      <td>تامر حسين</td>\n",
       "      <td>بيوحشنا</td>\n",
       "      <td>[ملازمنا, ملازمنا, خياله, وطيفه, فين, نروح, مل...</td>\n",
       "      <td>Early 2020s</td>\n",
       "      <td>محمد</td>\n",
       "      <td>أحمد فؤاد</td>\n",
       "      <td>تامر</td>\n",
       "      <td>حسين</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>أحمد إبراهيم</td>\n",
       "      <td>أيمن بهجت قمر</td>\n",
       "      <td>معرفش حد بالأسم ده</td>\n",
       "      <td>[اعرفش, بالاسم, تاه, عقله, ولقاه, اعرفش, بالاس...</td>\n",
       "      <td>Early 2020s</td>\n",
       "      <td>أحمد</td>\n",
       "      <td>إبراهيم</td>\n",
       "      <td>أيمن</td>\n",
       "      <td>بهجت قمر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد يحيي</td>\n",
       "      <td>بهاء الدين محمد</td>\n",
       "      <td>ظبط مودها</td>\n",
       "      <td>[تظبط, مودها, اطلب, عينيها, تاخدها, واؤمر, واح...</td>\n",
       "      <td>Early 2020s</td>\n",
       "      <td>محمد</td>\n",
       "      <td>يحيي</td>\n",
       "      <td>بهاء</td>\n",
       "      <td>الدين محمد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد يحيي</td>\n",
       "      <td>محمد القاياتي</td>\n",
       "      <td>سلامك وصلي</td>\n",
       "      <td>[سلامك, وصلي, واتاريني, واحشك, واحشني, وهتفضل,...</td>\n",
       "      <td>Early 2020s</td>\n",
       "      <td>محمد</td>\n",
       "      <td>يحيي</td>\n",
       "      <td>محمد</td>\n",
       "      <td>القاياتي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>محمد يحيي</td>\n",
       "      <td>محمد البوغة</td>\n",
       "      <td>واخدين راحتهم</td>\n",
       "      <td>[واخذين, راحتهم, قاعدين, قلبي, مربعين, وبيعصرو...</td>\n",
       "      <td>Early 2020s</td>\n",
       "      <td>محمد</td>\n",
       "      <td>يحيي</td>\n",
       "      <td>محمد</td>\n",
       "      <td>البوغة</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year        Composer         Lyricist                Song  \\\n",
       "0  2023  محمد أحمد فؤاد        تامر حسين             بيوحشنا   \n",
       "1  2023    أحمد إبراهيم    أيمن بهجت قمر  معرفش حد بالأسم ده   \n",
       "2  2023       محمد يحيي  بهاء الدين محمد           ظبط مودها   \n",
       "3  2023       محمد يحيي    محمد القاياتي          سلامك وصلي   \n",
       "4  2023       محمد يحيي      محمد البوغة       واخدين راحتهم   \n",
       "\n",
       "                                              Lyrics       Decade  \\\n",
       "0  [ملازمنا, ملازمنا, خياله, وطيفه, فين, نروح, مل...  Early 2020s   \n",
       "1  [اعرفش, بالاسم, تاه, عقله, ولقاه, اعرفش, بالاس...  Early 2020s   \n",
       "2  [تظبط, مودها, اطلب, عينيها, تاخدها, واؤمر, واح...  Early 2020s   \n",
       "3  [سلامك, وصلي, واتاريني, واحشك, واحشني, وهتفضل,...  Early 2020s   \n",
       "4  [واخذين, راحتهم, قاعدين, قلبي, مربعين, وبيعصرو...  Early 2020s   \n",
       "\n",
       "  Composer_first_name Composer_last_name Lyricist_first_name  \\\n",
       "0                محمد          أحمد فؤاد                تامر   \n",
       "1                أحمد            إبراهيم                أيمن   \n",
       "2                محمد               يحيي                بهاء   \n",
       "3                محمد               يحيي                محمد   \n",
       "4                محمد               يحيي                محمد   \n",
       "\n",
       "  Lyricist_last_name  \n",
       "0               حسين  \n",
       "1           بهجت قمر  \n",
       "2         الدين محمد  \n",
       "3           القاياتي  \n",
       "4             البوغة  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the processed data\n",
    "songs.to_csv(PREPROC_PATH, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Display processed data\n",
    "songs.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
